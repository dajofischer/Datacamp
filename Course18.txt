ty affiliation of United States Congressmen based on their voting records.


Supervised learning
50 XP

Which of these is a classification problem?
50 XP

Exploratory data analysis
50 XP

Numerical EDA
50 XP

Visual EDA
50 XP

The classification challenge
50 XP

k-Nearest Neighbors: Fit
100 XP

k-Nearest Neighbors: Predict
100 XP

Measuring model performance
50 XP

The digits recognition dataset
100 XP

Train/Test Split + Fit/Predict/Accuracy
100 XP

Overfitting and underfitting
100 XP
Hide Details
2
Regression
0%
In the previous chapter, you made use of image and political datasets to predict binary as well as multiclass outcomes. But what if your problem requires a continuous outcome? Regression, which is the focus of this chapter, is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data.


Introduction to regression
50 XP

Which of the following is a regression problem?
50 XP

Importing data for supervised learning
100 XP

Exploring the Gapminder data
50 XP

The basics of linear regression
50 XP

Fit & predict for regression
100 XP

Train/test split for regression
100 XP

Cross-validation
50 XP

5-fold cross-validation
100 XP

K-Fold CV comparison
100 XP

Regularized regression
50 XP

Regularization I: Lasso
100 XP

Regularization II: Ridge
100 XP
Hide Details
3
Fine-tuning your model
0%
Having trained your model, your next task is to evaluate its performance. What metrics can you use to gauge how good your model is? So far, you have used accuracy for classification and R-squared for regression. In this chapter, you will learn about some of the other metrics available in scikit-learn that will allow you to assess your model's performance in a more nuanced manner. You will then learn to optimize both your classification as well as regression models using hyperparameter tuning.


How good is your model?
50 XP

Metrics for classification
100 XP

Logistic regression and the ROC curve
50 XP

Building a logistic regression model
100 XP

Plotting an ROC curve
100 XP

Precision-recall Curve
50 XP

Area under the ROC curve
50 XP

AUC computation
100 XP

Hyperparameter tuning
50 XP

Hyperparameter tuning with GridSearchCV
100 XP

Hyperparameter tuning with RandomizedSearchCV
100 XP

Hold-out set for final evaluation
50 XP

Hold-out set reasoning
50 XP

Hold-out set in practice I: Classification
100 XP

Hold-out set in practice II: Regression
100 XP
Hide Details
4
Preprocessing and pipelines
0%
This chapter will introduce the notion of pipelines and how scikit-learn allows for transformers and estimators to be chained together and used as a single unit. Pre-processing techniques will be then be introduced as a way to enhance model performance and pipelines will be the glue that ties together concepts in the prior chapters.


Preprocessing data
50 XP

Exploring categorical features
100 XP

Creating dummy variables
100 XP

Regression with categorical features
100 XP

Handling missing data
50 XP

Dropping missing data
100 XP

Imputing missing data in a ML Pipeline I
100 XP

Imputing missing data in a ML Pipeline II
100 XP

Centering and scaling
50 XP

Centering and scaling your data
100 XP

Centering and scaling in a pipeline
100 XP

Bringing it all together I: Pipeline for classification
100 XP

Bringing it all together II: Pipeline for regression
100 XP

Final thoughts
50 XP
